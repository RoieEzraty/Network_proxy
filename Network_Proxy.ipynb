{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709f626e",
   "metadata": {},
   "source": [
    "# Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install nb_mypy\n",
    "# %load_ext nb_mypy\n",
    "# %nb_mypy On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from typing import Any, Tuple, List, Dict, Set"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "20a56a2a",
   "metadata": {},
   "source": [
    "Allostery:\n",
    "<div>\n",
    "<img src=\"attachment:image-2.png\" width=\"300\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ugly Classes\n",
    "\n",
    "class Syms():\n",
    "    def __init__(self, variabs):\n",
    "        super(Syms, self).__init__()\n",
    "        if variabs.task_type == 'Allostery':\n",
    "            self.x, self.y, self.R1, self.R2, self.Rl, self.p = sp.symbols('x, y, R1, R2, Rl, p')    \n",
    "        elif variabs.task_type == 'Regression':\n",
    "            self.x, self.R1, self.R2, self.Rl, self.p1, self.p2 = sp.symbols('x, R1, R2, Rl, p1, p2') \n",
    "        elif variabs.task_type == 'General_reg':\n",
    "            self.x, self.y, self.z, self.R1, self.R2, self.R3, \\\n",
    "            self.R4, self.R5, self.R6, self.p1, self.p2 = sp.symbols('x, y, z, R1, R2, R3, R4, R5, R6, p1, p2') \n",
    "        elif variabs.task_type == 'General_reg_allRsChange':\n",
    "            self.x, self.y, self.z, self.R1, self.R2, self.R3, self.R4, self.R5, self.R6, self.R7, self.R8, self.R9, \\\n",
    "            self.R10, self.R11, self.R12, self.R13, self.R14, self.R15, self.p1, self.p2 = sp.symbols('x y z R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 p1 p2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classes\n",
    "\n",
    "class Variabs():\n",
    "    def __init__(self, task_type, alpha, gamma, use_p_tag=True, R_update: str = 'propto'):\n",
    "        super(Variabs, self).__init__()\n",
    "        self.task_type = task_type\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.use_p_tag = use_p_tag \n",
    "        self.R_update = R_update  # 'propto' if R=gamma*delta_p\n",
    "                                  # 'deltaR' if deltaR=gamma*delta_p, gamma should be small\n",
    "\n",
    "\n",
    "class State():\n",
    "    def __init__(self, syms, variabs, supress_prints, bc_noise):\n",
    "        super(State, self).__init__()\n",
    "        self.syms = syms\n",
    "        self.variabs = variabs\n",
    "        \n",
    "        # self.R_in_t = [np.array([2.59272486, 5.18544973])]\n",
    "        self.out_in_t = []\n",
    "        self.loss_in_t = []\n",
    "        self.t = 0\n",
    "        \n",
    "        # inital resistance, out_dual and p_in\n",
    "        if self.variabs.task_type == 'Allostery':\n",
    "            self.R_in_t = [np.array([7.,7.])]\n",
    "            self.out_dual_in_t = [np.array([0.5,0.5])]\n",
    "            self.p_in_t = [1]\n",
    "        elif self.variabs.task_type == 'Regression':\n",
    "            self.R_in_t = [np.array([1.,1.])]\n",
    "            self.out_dual_in_t = [np.array([0.5])]\n",
    "            # self.p_in_t = [np.array([0.5, 1.5])]\n",
    "            self.p_in_t = [np.array([1, 1])]\n",
    "        elif self.variabs.task_type == 'General_reg':\n",
    "            self.R_in_t = [np.array([1.,1.,1.,1.,1.,1.])]\n",
    "            # self.R_in_t = [np.array([18.7,2.4,4.0,-13.5,-12.8,1.5])]  # theoretical one\n",
    "            # self.R_in_t = [np.array([13.8,7.3,-1.5,-8.0,-2.4,-8.9])]  # solved using pinv as in matlab code\n",
    "                                                                        # \"Calculate_desired_resistances_GeneralReg2024.m\"\n",
    "            self.out_dual_in_t = [0.5*np.ones(3)]\n",
    "            self.p_in_t = [1.0*np.ones(2)]\n",
    "        elif self.variabs.task_type == 'General_reg_allRsChange':\n",
    "            self.R_in_t = [np.ones((15), dtype=float)]\n",
    "            self.out_dual_in_t = [0.5*np.ones(3)]\n",
    "            self.p_in_t = [1.0*np.ones(2)]\n",
    "            \n",
    "        # initalized drawn sample vec and loss func\n",
    "        if self.variabs.task_type == 'Allostery':\n",
    "            self.loss_fn = loss_fn_allostery\n",
    "        else:\n",
    "            self.p_drawn_in_t = []\n",
    "            self.desired_in_t = []\n",
    "            self.loss_fn = loss_fn_regression\n",
    "        self.supress_prints = supress_prints\n",
    "        self.bc_noise=bc_noise\n",
    "        \n",
    "    def assign_P(self, syms):\n",
    "        if self.variabs.task_type == 'Allostery':\n",
    "            self.P = (syms.p-syms.x)**2/syms.R1 + (syms.p-syms.y)**2/syms.R2 + syms.x**2 + syms.y**2 + \\\n",
    "                     syms.p**2/syms.Rl + (syms.x-syms.y)**2/syms.Rl \n",
    "        elif self.variabs.task_type == 'Regression':\n",
    "            self.P = (syms.p1-syms.x)**2/syms.R1 + (syms.p2-syms.x)**2/syms.R2 + syms.p1**2 + syms.p2**2 + \\\n",
    "                     syms.x**2/syms.Rl + (syms.p1-syms.p2)**2/syms.Rl\n",
    "        elif self.variabs.task_type == 'General_reg':\n",
    "            self.P = (syms.p1-syms.x)**2/syms.R1 + (syms.p2-syms.x)**2/syms.R2 + (syms.p1-syms.y)**2/syms.R3 + \\\n",
    "                     (syms.p2-syms.y)**2/syms.R4 + (syms.p1-syms.z)**2/syms.R5 + (syms.p2-syms.z)**2/syms.R6 + \\\n",
    "                     syms.p1**2 + syms.p2**2 + syms.x**2 + syms.y**2 + syms.z**2 + (syms.p1-syms.p2)**2 + \\\n",
    "                     (syms.x-syms.y)**2 + (syms.y-syms.z)**2 + (syms.x-syms.z)**2\n",
    "        elif self.variabs.task_type == 'General_reg_allRsChange':\n",
    "            self.P = (syms.p1-syms.x)**2/syms.R1 + (syms.p2-syms.x)**2/syms.R2 + (syms.p1-syms.y)**2/syms.R3 + \\\n",
    "                     (syms.p2-syms.y)**2/syms.R4 + (syms.p1-syms.z)**2/syms.R5 + (syms.p2-syms.z)**2/syms.R6 + \\\n",
    "                     syms.p1**2/syms.R7 + syms.p2**2/syms.R8 + syms.x**2/syms.R9 + syms.y**2/syms.R10 + \\\n",
    "                     syms.z**2/syms.R11 + (syms.p1-syms.p2)**2/syms.R12 + (syms.x-syms.y)**2/syms.R13 + \\\n",
    "                     (syms.y-syms.z)**2/syms.R14 + (syms.x-syms.z)**2/syms.R15\n",
    "            \n",
    "    def solve_pressure(self, syms):        \n",
    "        if self.variabs.task_type == 'Allostery':\n",
    "            dPdx = self.P.diff(syms.x)\n",
    "            dPdy = self.P.diff(syms.y)\n",
    "            sols = sp.solve([dPdx,dPdy],[syms.x,syms.y])\n",
    "            self.x = sols[syms.x]\n",
    "            self.y = sols[syms.y]\n",
    "        elif self.variabs.task_type == 'Regression':\n",
    "            dPdx = self.P.diff(syms.x)\n",
    "            sols = sp.solve([dPdx],[syms.x])\n",
    "            self.x = sols[syms.x]\n",
    "        elif self.variabs.task_type == 'General_reg' or self.variabs.task_type == 'General_reg_allRsChange':\n",
    "            dPdx = self.P.diff(syms.x)\n",
    "            dPdy = self.P.diff(syms.y)\n",
    "            dPdz = self.P.diff(syms.z)\n",
    "            sols = sp.solve([dPdx,dPdy,dPdz],[syms.x,syms.y,syms.z])\n",
    "            self.x = sols[syms.x]\n",
    "            self.y = sols[syms.y]\n",
    "            self.z = sols[syms.z]\n",
    "    \n",
    "    def calc_output(self, syms):\n",
    "        # substitute the last resistances and initial input pressure in theoretical expression for pressure at output\n",
    "        if self.variabs.task_type == 'Allostery':\n",
    "            x_nxt = self.x.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.p:self.p_in_t[0],\n",
    "                                 syms.Rl:2**(1/2)})\n",
    "            y_nxt = self.y.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.p:self.p_in_t[0],\n",
    "                                 syms.Rl:2**(1/2)})\n",
    "            self.output = np.array([x_nxt, y_nxt])\n",
    "        elif self.variabs.task_type == 'Regression':\n",
    "            x_nxt = self.x.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.p1:self.p_drawn[0],\n",
    "                                 syms.p2:self.p_drawn[1], syms.Rl:2**(1/2)})\n",
    "            self.output = x_nxt\n",
    "        elif self.variabs.task_type == 'General_reg':\n",
    "            x_nxt = self.x.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.R3:self.R_in_t[-1][2],\\\n",
    "                                 syms.R4:self.R_in_t[-1][3], syms.R5:self.R_in_t[-1][4], syms.R6:self.R_in_t[-1][5],\\\n",
    "                                 syms.p1:self.p_drawn[0],syms.p2:self.p_drawn[1]})\n",
    "            y_nxt = self.y.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.R3:self.R_in_t[-1][2],\\\n",
    "                                 syms.R4:self.R_in_t[-1][3], syms.R5:self.R_in_t[-1][4], syms.R6:self.R_in_t[-1][5],\\\n",
    "                                 syms.p1:self.p_drawn[0],syms.p2:self.p_drawn[1]})\n",
    "            z_nxt = self.z.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.R3:self.R_in_t[-1][2],\\\n",
    "                                 syms.R4:self.R_in_t[-1][3], syms.R5:self.R_in_t[-1][4], syms.R6:self.R_in_t[-1][5],\\\n",
    "                                 syms.p1:self.p_drawn[0],syms.p2:self.p_drawn[1]})\n",
    "            self.output = np.array([x_nxt, y_nxt, z_nxt])\n",
    "        elif self.variabs.task_type == 'General_reg_allRsChange':\n",
    "            x_nxt = self.x.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.R3:self.R_in_t[-1][2],\\\n",
    "                                 syms.R4:self.R_in_t[-1][3], syms.R5:self.R_in_t[-1][4], syms.R6:self.R_in_t[-1][5],\\\n",
    "                                 syms.R7:self.R_in_t[-1][6], syms.R8:self.R_in_t[-1][7], syms.R9:self.R_in_t[-1][8],\\\n",
    "                                 syms.R10:self.R_in_t[-1][9], syms.R11:self.R_in_t[-1][10], syms.R12:self.R_in_t[-1][11],\\\n",
    "                                 syms.R13:self.R_in_t[-1][12], syms.R14:self.R_in_t[-1][13], syms.R15:self.R_in_t[-1][14],\\\n",
    "                                 syms.p1:self.p_drawn[0],syms.p2:self.p_drawn[1]})\n",
    "            y_nxt = self.y.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.R3:self.R_in_t[-1][2],\\\n",
    "                                 syms.R4:self.R_in_t[-1][3], syms.R5:self.R_in_t[-1][4], syms.R6:self.R_in_t[-1][5],\\\n",
    "                                 syms.R7:self.R_in_t[-1][6], syms.R8:self.R_in_t[-1][7], syms.R9:self.R_in_t[-1][8],\\\n",
    "                                 syms.R10:self.R_in_t[-1][9], syms.R11:self.R_in_t[-1][10], syms.R12:self.R_in_t[-1][11],\\\n",
    "                                 syms.R13:self.R_in_t[-1][12], syms.R14:self.R_in_t[-1][13], syms.R15:self.R_in_t[-1][14],\\\n",
    "                                 syms.p1:self.p_drawn[0],syms.p2:self.p_drawn[1]})\n",
    "            z_nxt = self.z.subs({syms.R1:self.R_in_t[-1][0], syms.R2:self.R_in_t[-1][1], syms.R3:self.R_in_t[-1][2],\\\n",
    "                                 syms.R4:self.R_in_t[-1][3], syms.R5:self.R_in_t[-1][4], syms.R6:self.R_in_t[-1][5],\\\n",
    "                                 syms.R7:self.R_in_t[-1][6], syms.R8:self.R_in_t[-1][7], syms.R9:self.R_in_t[-1][8],\\\n",
    "                                 syms.R10:self.R_in_t[-1][9], syms.R11:self.R_in_t[-1][10], syms.R12:self.R_in_t[-1][11],\\\n",
    "                                 syms.R13:self.R_in_t[-1][12], syms.R14:self.R_in_t[-1][13], syms.R15:self.R_in_t[-1][14],\\\n",
    "                                 syms.p1:self.p_drawn[0],syms.p2:self.p_drawn[1]})\n",
    "            self.output = np.array([x_nxt, y_nxt, z_nxt])\n",
    "            \n",
    "        # display and save output in time\n",
    "        if self.supress_prints:\n",
    "            pass\n",
    "        else:\n",
    "            print('output=', self.output)\n",
    "        self.out_in_t.append(self.output)\n",
    "    \n",
    "    def calc_loss(self, desired, output_prev=0, desired_prev=0):\n",
    "        if type(output_prev)==int and type(desired)!=np.float64:  # if desired not scalar, must change other sizes as well\n",
    "            output_prev = np.zeros(len(desired))\n",
    "            desired_prev = np.zeros(len(desired))\n",
    "        if self.variabs.task_type=='Allostery':\n",
    "            self.loss = self.loss_fn(self.output, desired)\n",
    "        elif self.variabs.task_type=='Regression' or self.variabs.task_type=='General_reg' \\\n",
    "             or self.variabs.task_type=='General_reg_allRsChange':\n",
    "            self.loss = self.loss_fn(self.output, output_prev, desired, desired_prev)\n",
    "        self.loss_in_t.append(self.loss)\n",
    "        \n",
    "    def update_pressure(self):\n",
    "        self.t += 1\n",
    "        loss = self.loss_in_t[-1]\n",
    "        p = self.p_in_t[-1]\n",
    "        pert = np.random.normal(size=np.size(p))\n",
    "        if self.variabs.task_type == 'Allostery':     \n",
    "            p_nxt = p - np.dot(self.variabs.alpha, loss+self.bc_noise*pert)\n",
    "        elif self.variabs.task_type == 'Regression' or self.variabs.task_type=='General_reg' \\\n",
    "             or self.variabs.task_type=='General_reg_allRsChange':\n",
    "            p_drawn = self.p_drawn_in_t[-1]\n",
    "            if self.variabs.use_p_tag:\n",
    "                p_drawn_prev = self.p_drawn_in_t[-2]\n",
    "                p_nxt = p - (p_drawn-p_drawn_prev)*np.dot(self.variabs.alpha, loss[0]-loss[1])\n",
    "#             p_nxt = p - np.dot(self.variabs.alpha, np.dot(p_drawn-p_drawn_prev, loss[0]-loss[1]))\n",
    "#             print('the dot for p', np.dot(self.variabs.alpha, np.dot(p_drawn-p_drawn_prev, loss[0]-loss[1])))\n",
    "                print('the dot for p', np.dot(self.variabs.alpha, loss[0]-loss[1]))\n",
    "            else:\n",
    "                p_nxt = p - (p_drawn)*np.dot(self.variabs.alpha, loss[0])          \n",
    "                print('the dot for p', np.dot(self.variabs.alpha, loss[0]))            \n",
    "        if self.supress_prints:\n",
    "            pass\n",
    "        else:\n",
    "            print('loss=', loss)\n",
    "            print('time=', self.t)\n",
    "            print('p_nxt=', p_nxt)\n",
    "        \n",
    "        # pressure changes without memory?\n",
    "        if self.variabs.R_update == 'deltaR' and np.shape(self.p_in_t)[0]>1:  # make sure its not initial value\n",
    "            p_nxt -= p  # erase memory\n",
    "        self.p_in_t.append(p_nxt)\n",
    "        \n",
    "    def update_BCs(self, desired):\n",
    "        loss = self.loss_in_t[-1]\n",
    "        pert = np.random.normal(size=np.size(self.output))\n",
    "        out_dual = self.out_dual_in_t[-1]\n",
    "        if self.variabs.task_type=='Allostery':\n",
    "            self.output = out_dual + self.variabs.alpha * (loss + self.bc_noise*pert)\n",
    "        elif self.variabs.task_type=='Regression' or self.variabs.task_type=='General_reg' \\\n",
    "        or self.variabs.task_type=='General_reg_allRsChange':\n",
    "            # self.output = out_dual + self.variabs.alpha * np.dot(self.output-self.out_in_t[-2], loss[0]-loss[1])\n",
    "            self.output = out_dual + self.variabs.alpha * (self.output-self.out_in_t[-2])*(loss[0]-loss[1])\n",
    "        \n",
    "        # BCs change without memory?\n",
    "        if self.variabs.R_update == 'deltaR' and np.shape(self.out_dual_in_t)[0]>1:  # make sure its not initial value\n",
    "            self.output -= out_dual  # erase memory\n",
    "        self.out_dual_in_t.append(self.output)           \n",
    "        if self.supress_prints:\n",
    "            pass\n",
    "        else:\n",
    "            print('dual output', self.output)\n",
    "    \n",
    "    def update_Rs(self):\n",
    "        if self.variabs.task_type=='Allostery' or self.variabs.task_type=='Regression':\n",
    "            self.R_in_t.append(self.variabs.gamma * (self.p_in_t[-1]-self.output))\n",
    "        elif self.variabs.task_type=='General_reg':\n",
    "            R_ij = []\n",
    "            print(self.p_in_t[-1])\n",
    "            for i, p in enumerate(self.p_in_t[-1]):\n",
    "                for j, out in enumerate(self.output):\n",
    "                    R_ij.append(self.variabs.gamma[i*len(self.output)+j] * (p - out))\n",
    "            self.R_in_t.append(R_ij)\n",
    "            # self.R_in_t.append(self.R_in_t[0])\n",
    "        elif self.variabs.task_type=='General_reg_allRsChange':\n",
    "            R_ij = []\n",
    "            # inputs to outputs\n",
    "            for i, p in enumerate(self.p_in_t[-1]):\n",
    "                for j, out in enumerate(self.output):\n",
    "                    R_ij.append(self.variabs.gamma[i*len(self.output)+j] * (p - out))\n",
    "            # input to ground\n",
    "            for i, p in enumerate(self.p_in_t[-1]):\n",
    "                R_ij.append(self.variabs.gamma[i] * (p - 0))\n",
    "            # outputs to ground\n",
    "            for j, out, in enumerate(self.output):\n",
    "                R_ij.append(self.variabs.gamma[j] * (out - 0))\n",
    "            # inputs between themselves\n",
    "            R_ij.append(self.variabs.gamma[0] * (self.p_in_t[-1][0] - self.p_in_t[-1][1]))\n",
    "            # outputs between themselves\n",
    "            for i, out in enumerate(self.output):\n",
    "                R_ij.append(self.variabs.gamma[0] * (self.output[i] - self.output[(i+1)%3]))\n",
    "            self.R_in_t.append(R_ij)\n",
    "        \n",
    "        # if deltaR=gamma*delta_p\n",
    "        if self.variabs.R_update == 'deltaR' and np.shape(self.R_in_t)[0]>1:  # make sure its not initial value\n",
    "            print('update is deltaR')\n",
    "            self.R_in_t[-1] += self.R_in_t[-2]  # update previous value [-2], gamma should be small\n",
    "            \n",
    "        # optionally display resistances\n",
    "        if self.supress_prints:\n",
    "            pass\n",
    "        else:\n",
    "            print('R_nxt', self.R_in_t[-1])\n",
    "            \n",
    "    def draw_p(self):\n",
    "        self.p_drawn = np.random.uniform(low=0.0, high=2.0, size=2)\n",
    "        self.p_drawn_in_t.append(self.p_drawn)\n",
    "        # self.p_in_t[-1] = p_drawn\n",
    "        # x=5\n",
    "        \n",
    "    def update_alpha(self, alpha):\n",
    "        if self.variabs.task_type == 'Allostery':\n",
    "            self.variabs.alpha = np.array([alpha, alpha])\n",
    "        else:\n",
    "            self.variabs.alpha = alpha\n",
    "        # print('new alpha=', self.variabs.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6217b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "\n",
    "def run_model_changing_alpha(alpha_vec, task_type, training, supress, bc_noise=0, use_p_tag=True, R_update: str ='propto'):\n",
    "    if task_type=='Allostery':\n",
    "        alpha_vec = np.column_stack((alpha_vec, alpha_vec))\n",
    "        print('alpha_vec', alpha_vec)\n",
    "        gamma = -np.array([0.5, 0.5])  # how much resistance changed due to pressure difference\n",
    "        # gamma = -np.array([1.0, 1.0])  # how much resistance changed due to pressure difference\n",
    "        x_hat = 0.2\n",
    "        y_hat = 0.3\n",
    "        desired = np.array([x_hat, y_hat])\n",
    "        A = copy.copy(x_hat)\n",
    "        B = copy.copy(y_hat)\n",
    "    elif task_type=='Regression':\n",
    "        gamma = np.array([1.0])  # how much resistance changed due to pressure difference\n",
    "        A = 0.4\n",
    "        B = 0.25\n",
    "        AB_vec = np.array([A, B])  # this is in terms of A & B, the calculated desired pressure is calculated in train loop\n",
    "    elif task_type=='General_reg' or task_type=='General_reg_allRsChange':\n",
    "        alpha_vec = np.column_stack((alpha_vec, alpha_vec, alpha_vec))\n",
    "        print(alpha_vec)\n",
    "        gamma = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])  # how much resistance changed due to pressure difference\n",
    "        A = 0.05\n",
    "        B = 0.25\n",
    "        C = 0.1\n",
    "        D = 0.15\n",
    "        E = 0.02\n",
    "        F = 0.3\n",
    "        M = np.array([[A, B],  \n",
    "                      [C, D], \n",
    "                      [E, F]])  # this is in terms of M matrix, desired press. is calculated in train loop\n",
    "    \n",
    "    for j, alpha in enumerate(alpha_vec):\n",
    "#        print(alpha)\n",
    "        ## classes for variables and symbols\n",
    "        variabs = Variabs(task_type, alpha, gamma, use_p_tag=use_p_tag, R_update=R_update)\n",
    "        syms = Syms(variabs)\n",
    "\n",
    "        ## network state\n",
    "        state = State(syms, variabs, supress_prints=supress, bc_noise=bc_noise)\n",
    "        state.assign_P(syms)\n",
    "        state.solve_pressure(syms)\n",
    "        \n",
    "        for i in range(training):\n",
    "            if task_type=='Allostery':\n",
    "                if np.size(alpha)>2:  # if alpha is annealed, change. In allostery alpha is 2D anyway\n",
    "                    state.update_alpha(alpha[i])\n",
    "                state.calc_output(syms)\n",
    "                state.calc_loss(desired)\n",
    "                state.update_pressure()\n",
    "                state.update_BCs(desired)\n",
    "                state.update_Rs()\n",
    "            elif task_type=='Regression' or task_type=='General_reg' or task_type=='General_reg_allRsChange':\n",
    "                if task_type=='Regression' and np.size(alpha)>1 :\n",
    "                    state.update_alpha(alpha(i))\n",
    "                state.draw_p()\n",
    "                if task_type=='Regression':\n",
    "                    desired = np.dot(AB_vec, state.p_drawn)\n",
    "                else:\n",
    "                    desired = np.matmul(M, state.p_drawn)\n",
    "                state.desired_in_t.append(desired)\n",
    "                if supress:\n",
    "                    pass\n",
    "                else:\n",
    "                    print('p_drawn', state.p_drawn)\n",
    "                    print('desired', desired)\n",
    "                state.calc_output(syms)\n",
    "                if not i % 2:  # even iterations\n",
    "                    pass\n",
    "                else:\n",
    "                    if use_p_tag:\n",
    "                        state.calc_loss(desired, state.out_in_t[-2], state.desired_in_t[-2])\n",
    "                    else:\n",
    "                        state.calc_loss(desired)\n",
    "                    state.update_pressure()\n",
    "                    state.update_BCs(desired)\n",
    "                    state.update_Rs()\n",
    "        # print(state.p_in_t)\n",
    "        plot_importants(state, variabs, desired, A, B)\n",
    "    return state, variabs, desired\n",
    "\n",
    "\n",
    "def plot_importants(state, variabs, desired, A=1, B=1):\n",
    "    if variabs.task_type == 'Allostery':\n",
    "        A = desired[0]/state.p_in_t[0]  # A = x_hat/p_in\n",
    "        B = desired[1]/state.p_in_t[0]  # B = y_hat/p_in\n",
    "        Rl_subs = 2**(1/2)\n",
    "        R_theor = state.p_in_t[0]*np.array([(1-A)/(A*(1+1/Rl_subs)-B/Rl_subs), (1-B)/(B*(1+1/Rl_subs)-A/Rl_subs)])\n",
    "        legend1 = [r'$\\frac{x}{x\\,\\mathrm{desired}}$', r'$\\frac{y}{y\\,\\mathrm{desired}}$']\n",
    "        legend2 = [r'$x\\,\\mathrm{dual}$', r'$y\\,\\mathrm{dual}$', r'$p\\,\\mathrm{dual}$']\n",
    "        legend3 = [r'$R_1$', r'$R_2$', r'$R_1\\,\\mathrm{theoretical}$', r'$R_2\\,\\mathrm{theoretical}$']\n",
    "    elif variabs.task_type == 'Regression':\n",
    "        Rl_subs = 2**(1/2)\n",
    "        R_theor = np.array([Rl_subs*(1-A-B)/A, Rl_subs*(1-A-B)/B])\n",
    "        legend1 = [r'$\\frac{x}{x\\,\\mathrm{desired}}$']\n",
    "        legend2 = [r'$x\\,\\mathrm{dual}$', r'$p_1\\,\\mathrm{dual}$', r'$p_2\\,\\mathrm{dual}$']\n",
    "        legend3 = [r'$R_1$', r'$R_2$', r'$R_1\\,\\mathrm{theoretical}$', r'$R_2\\,\\mathrm{theoretical}$']\n",
    "    elif variabs.task_type=='General_reg' or variabs.task_type=='General_reg_allRsChange':\n",
    "        A = desired[0]/state.p_in_t[0]  # A = x_hat/p_in\n",
    "        B = desired[1]/state.p_in_t[0]  # B = y_hat/p_in\n",
    "#         C = desired[2]/state.p_in_t[0]  # B = y_hat/p_in\n",
    "#         D = desired[3]/state.p_in_t[0]  # B = y_hat/p_in\n",
    "#         E = desired[4]/state.p_in_t[0]  # B = y_hat/p_in\n",
    "#         F = desired[5]/state.p_in_t[0]  # B = y_hat/p_in\n",
    "        # R_theor = state.p_in_t[0]*np.array([(1-A)/(A*(1+1/Rl_subs)-B/Rl_subs), (1-B)/(B*(1+1/Rl_subs)-A/Rl_subs)])\n",
    "        # legend1 = ['x', 'y', 'z', 'x desired', 'y deisred', 'z desired']\n",
    "        legend1 = [r'$\\frac{x}{x\\,\\mathrm{desired}}$', r'$\\frac{y}{y\\,\\mathrm{desired}}$', \\\n",
    "                   r'$\\frac{z}{z\\,\\mathrm{desired}}$']\n",
    "        legend2 = [r'$x\\,\\mathrm{dual}$', r'$y\\,\\mathrm{dual}$', r'$z\\,\\mathrm{dual}$', r'$p_1\\,\\mathrm{dual}$',\n",
    "                   r'$p_2\\,\\mathrm{dual}$']\n",
    "        legend3 = [r'$R_1$', r'$R_2$', r'$R_3$', r'$R_4$', r'$R_5$', r'$R_6$']\n",
    "        R_theor = []  # I didn't calculate it for this task\n",
    "    legend4 = ['|loss|']\n",
    "    print('R theoretical', R_theor)\n",
    "\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(12, 3.2))\n",
    "    if variabs.task_type=='Allostery':\n",
    "        ax1.plot(state.out_in_t[1:]/desired-1)\n",
    "        ax1.plot(np.zeros(len(state.out_in_t[1:])), '--')\n",
    "    # regression goes only every two samples\n",
    "    elif variabs.task_type=='Regression' or variabs.task_type=='General_reg' or variabs.task_type=='General_reg_allRsChange':\n",
    "        ax1.plot(np.linspace(0, state.t, 2*state.t-1).T, \\\n",
    "                 np.asarray(state.out_in_t[1:])/np.asarray(state.desired_in_t[1:])-1)\n",
    "    ax1.set_title('output in time')\n",
    "    ax1.set_xlabel('t')\n",
    "    ax1.legend(legend1)\n",
    "    ax2.plot(state.out_dual_in_t[1:])\n",
    "    ax2.plot(state.p_in_t[1:])\n",
    "    ax2.set_title('dual and p in time')\n",
    "    ax2.set_xlabel('t')\n",
    "    ax2.set_ylim([-0.2,0.2])\n",
    "    ax2.legend(legend2)\n",
    "    ax3.plot(state.R_in_t[1:])\n",
    "    ax3.plot(np.outer(R_theor,np.ones(state.t)).T, '--')\n",
    "    ax3.set_title('R in time')\n",
    "    ax3.set_xlabel('t')\n",
    "    ax3.legend(legend3)\n",
    "    if variabs.task_type=='Allostery' or variabs.task_type=='Regression':  # loss is 2D in Allostery\n",
    "        ax4.plot(np.abs(state.loss_in_t[1:]))\n",
    "    elif variabs.task_type=='General_reg' or variabs.task_type=='General_reg_allRsChange':    \n",
    "        ax4.plot(np.mean(np.mean(np.abs(state.loss_in_t[1:]), axis=1),axis=1))\n",
    "    ax4.set_xlabel('t')\n",
    "    ax4.legend(legend4)\n",
    "    fig.suptitle(f'alpha={variabs.alpha}')\n",
    "    plt.show()\n",
    "    # print(state.R_in_t)\n",
    "\n",
    "def loss_fn_allostery(output, desired):\n",
    "    # return np.sign(desired-output)\n",
    "    # return np.e**(-(desired-output)\n",
    "    return desired-output\n",
    "    # return np.sign(desired-output) * np.e**(-np.abs((desired-output)))\n",
    "    \n",
    "def loss_fn_regression(output1, output2, desired1, desired2):\n",
    "    L1 = desired1-output1\n",
    "    L2 = desired2-output2\n",
    "    return np.array([L1, L2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdb30c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "# learning rate\n",
    "# alpha_vec = np.array([0.2, 0.5, 1.0, 2.0, 2.5, 2.7, 3.5, 4.0, 8.0])  # 1D array = const alpha, array of arrays = annealed\n",
    "# alpha_vec = np.array([np.append(np.linspace(4.1,4.8,200), np.linspace(4.8,4.1,200))])  # 1D array for constant alpha, array of arrays for annealed alpha\n",
    "alpha_vec = np.array([-1.0])  # for allostery, regression and general_reg\n",
    "# alpha_vec = np.array([0.1])  # for general_reg_allRsChange\n",
    "# alpha_vec = np.array([np.linspace(5,3.85,40)])  # 1D array for constant alpha, array of arrays for annealed alpha\n",
    "# alpha_vec = np.array([4.8])\n",
    "\n",
    "# task - allostery or regression by now\n",
    "task_type: str =  'Allostery'\n",
    "# task_type: str = 'Regression'\n",
    "# task_type: str = 'General_reg'\n",
    "# task_type: str = 'General_reg_allRsChange'\n",
    "\n",
    "# R_update = 'propto'\n",
    "R_update = 'deltaR'\n",
    "\n",
    "# training length\n",
    "if np.size(alpha_vec[0])>1:  # if alpha is annealed\n",
    "    training: int = len(alpha_vec[0])\n",
    "else:\n",
    "    training = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b012068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training + plot\n",
    "\n",
    "state, variabs, desired = run_model_changing_alpha(alpha_vec, task_type, training, supress=False, bc_noise=0.0, \\\n",
    "                                                   use_p_tag=True, R_update=R_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86a98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "legends = [['x', 'x desired'], ['y', 'y desired'], ['z', 'z desired']]\n",
    "if variabs.task_type=='Allostery':\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "    for i in range(2):\n",
    "        axes[i].plot(np.linspace(0, state.t, state.t-1).T, np.vstack(state.out_in_t)[1:,i])\n",
    "        axes[i].plot(desired[i]*np.ones(state.t).T, '--')\n",
    "        axes[i].legend(legends[i])        \n",
    "    for i in range(2):\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(12, 4.5))\n",
    "        plt.plot(np.linspace(0, state.t, state.t-1).T, np.vstack(state.out_in_t)[1:,i])\n",
    "        plt.plot(desired[i]*np.ones(state.t).T, '--')\n",
    "        plt.legend(legends[i])\n",
    "elif variabs.task_type=='Regression':\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(12, 4.5))\n",
    "    axes.plot(np.linspace(0, state.t, 2*state.t-1).T, np.vstack(state.out_in_t)[1:])\n",
    "    axes.plot(np.linspace(0, state.t, 2*state.t).T, np.vstack(state.desired_in_t), '--')\n",
    "    axes.legend(legends[0])           \n",
    "# regression goes only every two samples\n",
    "elif task_type=='General_reg' or task_type=='General_reg_allRsChange':\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4.5))\n",
    "    for i in range(3):\n",
    "        axes[i].plot(np.linspace(0, state.t, 2*state.t-1).T, np.vstack(state.out_in_t)[1:,i])\n",
    "        axes[i].plot(np.linspace(0, state.t, 2*state.t).T, np.vstack(state.desired_in_t)[:,i], '--')\n",
    "        axes[i].legend(legends[i])        \n",
    "    for i in range(3):\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(12, 4.5))\n",
    "        plt.plot(np.linspace(0, state.t, 2*state.t-1).T, np.vstack(state.out_in_t)[1:,i])\n",
    "        plt.plot(np.linspace(0, state.t, 2*state.t).T, np.vstack(state.desired_in_t)[:,i], '--')\n",
    "        plt.legend(legends[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "loss_1d_in_t = np.mean(np.mean(np.abs(state.loss_in_t), axis=1), axis=1)\n",
    "\n",
    "# Convert the numpy array to a pandas Series\n",
    "loss_series = pd.Series(loss_1d_in_t)\n",
    "\n",
    "# Calculate the moving mean with a window size of 3\n",
    "moving_mean = loss_series.rolling(window=12).mean()\n",
    "\n",
    "# Convert back to numpy array if needed\n",
    "moving_mean_array = moving_mean.to_numpy()\n",
    "\n",
    "print(moving_mean_array)\n",
    "\n",
    "# plt.plot(loss_1d_in_t)\n",
    "plt.plot(moving_mean_array)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$Loss$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa915be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the hysteresis curve\n",
    "# it will work only if alpha is the same size as t\n",
    "\n",
    "if len(alpha_vec.T) == len(state.out_in_t):\n",
    "    plt.plot(alpha_vec.T, state.out_in_t, '.')\n",
    "    plt.plot(np.outer(desired,np.ones(state.t)).T, '--')\n",
    "    plt.xlim([min(alpha_vec[0])-0.01, max(alpha_vec[0]+0.01)])\n",
    "else:\n",
    "    plt.plot(state.out_in_t, '.')\n",
    "    plt.plot(np.outer(desired,np.ones(state.t)).T, '--')\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(r\"$x,y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(n: int) -> str:\n",
    "    \"\"\"Convert number to string.\n",
    "    \"\"\"\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7cbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
